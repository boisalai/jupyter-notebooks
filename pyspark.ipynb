{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc9700a",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "See installation instructions [here](https://github.com/boisalai/jupyter-notebooks/blob/main/pyspark.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0604c9",
   "metadata": {},
   "source": [
    "## Exemple 1: TLC Trip Record Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17bdd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd49f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-05 08:52:30--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Résolution de s3.amazonaws.com (s3.amazonaws.com)… 52.216.208.112, 52.216.217.200, 52.216.139.117, ...\n",
      "Connexion à s3.amazonaws.com (s3.amazonaws.com)|52.216.208.112|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 12322 (12K) [application/octet-stream]\n",
      "Sauvegarde en : « taxi+_zone_lookup.csv.3 »\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12,03K  --.-KB/s    ds 0,001s  \n",
      "\n",
      "2023-05-05 08:52:31 (23,3 MB/s) — « taxi+_zone_lookup.csv.3 » sauvegardé [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "668cc43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We now need to instantiate a Spark session, an object that we use to interact with Spark.\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f72506",
   "metadata": {},
   "source": [
    "## Exemple 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786366c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/homebrew/Cellar/apache-spark/3.4.0/libexec'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639bc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d6050a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/05 08:42:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "666457a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/05 08:42:46 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54941d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "columns = [\"language\", \"users_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feeb44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf48ac1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd1d7098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "/opt/homebrew/Cellar/apache-spark/3.4.0/libexec/python/lib/py4j-0.10.9.7-src.zip:/opt/homebrew/Cellar/apache-spark/3.4.0/libexec/python/:/opt/homebrew/Cellar/python@3.10/3.10.11\n"
     ]
    }
   ],
   "source": [
    "!echo $PYSPARK_PYTHON\n",
    "!echo $PYSPARK_DRIVER_PYTHON\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b51327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.3\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac27f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYSPARK_PYTHON=python3\n",
    "!export PYSPARK_DRIVER_PYTHON=python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1998570",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYSPARK_PYTHON=/opt/homebrew/Cellar/python@3.11\n",
    "!export PYSPARK_DRIVER_PYTHON=/opt/homebrew/Cellar/python@3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58b39a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "!export PYSPARK_DRIVER_PYTHON_OPTS=’lab’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24174e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" # path to java jdk\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7\" # path to spark home\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "# https://wenkangwei.github.io/2020/10/14/PySpark-0-Installation/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8431630",
   "metadata": {},
   "source": [
    "# Exemple 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414606d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main(spark):\n",
    "    df = spark.createDataFrame(\n",
    "        [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
    "        (\"id\", \"v\"))\n",
    "\n",
    "    @pandas_udf(\"double\")\n",
    "    def mean_udf(v: pd.Series) -> float:\n",
    "        return v.mean()\n",
    "\n",
    "    print(df.groupby(\"id\").agg(mean_udf(df['v'])).collect())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(SparkSession.builder.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.archives\",  # 'spark.yarn.dist.archives' in YARN.\n",
    "    \"pyspark_venv.tar.gz#environment\").getOrCreate()\n",
    "main(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
