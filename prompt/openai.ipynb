{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4002e4",
   "metadata": {},
   "source": [
    "# OpenAI\n",
    "\n",
    "See:\n",
    "\n",
    "* [Welcome to the OpenAI platform](https://platform.openai.com/overview)\n",
    "* [OpenAI Introduction](https://platform.openai.com/docs/introduction/overview)\n",
    "* [OpenAI Quickstart](https://platform.openai.com/docs/quickstart)\n",
    "* [Best practices for prompt engineering with OpenAI API](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)\n",
    "* [Usage Dashboard](https://platform.openai.com/account/usage)\n",
    "* https://github.com/openai/tiktoken\n",
    "* [Billing settings](https://platform.openai.com/account/billing/overview)\n",
    "\n",
    "See also: \n",
    "\n",
    "* [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6c1d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb==0.3.21 in /opt/homebrew/lib/python3.10/site-packages (0.3.21)\n",
      "Requirement already satisfied: tiktoken==0.3.3 in /opt/homebrew/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (0.20.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (0.6.2)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (1.10.4)\n",
      "Requirement already satisfied: sentence-transformers>=2.2.2 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (2.2.2)\n",
      "Requirement already satisfied: fastapi>=0.85.1 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (0.89.1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (1.23.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (3.0.1)\n",
      "Requirement already satisfied: pandas>=1.3 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (1.5.1)\n",
      "Requirement already satisfied: hnswlib>=0.7 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (0.7.0)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (0.8.1)\n",
      "Requirement already satisfied: requests>=2.28 in /opt/homebrew/lib/python3.10/site-packages (from chromadb==0.3.21) (2.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.10/site-packages (from tiktoken==0.3.3) (2022.10.31)\n",
      "Requirement already satisfied: lz4 in /opt/homebrew/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (4.3.2)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2022.9.24)\n",
      "Requirement already satisfied: urllib3>=1.26 in /opt/homebrew/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (1.26.12)\n",
      "Requirement already satisfied: pytz in /opt/homebrew/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2022.5)\n",
      "Requirement already satisfied: zstandard in /opt/homebrew/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (0.21.0)\n",
      "Requirement already satisfied: starlette==0.22.0 in /opt/homebrew/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.21) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/homebrew/lib/python3.10/site-packages (from starlette==0.22.0->fastapi>=0.85.1->chromadb==0.3.21) (3.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.21) (2.8.2)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/homebrew/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.21) (2.2.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.3.21) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.21) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.21) (2.1.1)\n",
      "Requirement already satisfied: nltk in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.2.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.15.1)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.9.3)\n",
      "Requirement already satisfied: sentencepiece in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.1.99)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/homebrew/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.30.2)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.12.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.1.3)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.19.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.17.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (11.0.3)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/homebrew/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.5.0)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2023.1.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (21.3)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.13.3)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/lib/python3.10/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/homebrew/lib/python3.10/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb==0.3.21) (9.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi>=0.85.1->chromadb==0.3.21) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/homebrew/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb==0.3.21 tiktoken==0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0d1fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebula: The Journey of Scaling Instacart’s Dat...</td>\n",
       "      <td>Instacart has gone through immense growth duri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Satellite Imaginary Data Processing Using Apac...</td>\n",
       "      <td>Agriculture is a complex ecosystem. Understand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From Snowflake to Enterprise-Scale Apache Spark™</td>\n",
       "      <td>Akamai mPulse is a real user monitoring (RUM) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Future of Data Orchestration: Asset-Based ...</td>\n",
       "      <td>Data orchestration is a core component for any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Photon for Dummies: How Does this New Executio...</td>\n",
       "      <td>Did you finish the Photon whitepaper and think...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Nebula: The Journey of Scaling Instacart’s Dat...   \n",
       "1  Satellite Imaginary Data Processing Using Apac...   \n",
       "2   From Snowflake to Enterprise-Scale Apache Spark™   \n",
       "3  The Future of Data Orchestration: Asset-Based ...   \n",
       "4  Photon for Dummies: How Does this New Executio...   \n",
       "\n",
       "                                            Abstract  \n",
       "0  Instacart has gone through immense growth duri...  \n",
       "1  Agriculture is a complex ecosystem. Understand...  \n",
       "2  Akamai mPulse is a real user monitoring (RUM) ...  \n",
       "3  Data orchestration is a core component for any...  \n",
       "4  Did you finish the Photon whitepaper and think...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "talks = pd.read_csv(\"dais23_talks.csv\")\n",
    "talks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01b868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse Abstract: Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform. Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include: - Modular and composable code - Unit testing framework - Incremental event processing with spark structured streaming - Granular resource tuning for better performance and cost efficacy Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse.\n"
     ]
    }
   ],
   "source": [
    "talks[\"full_text\"] = talks.apply(\n",
    "    lambda row: f\"\"\"Title: {row[\"Title\"]} Abstract: {row[\"Abstract\"]}\"\"\".strip(),\n",
    "    axis=1,\n",
    ")\n",
    "print(talks.iloc[0][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328693f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ~/test/chroma\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# See https://docs.trychroma.com/usage-guide#initiating-the-chroma-client\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",\n",
    "    persist_directory=\"~/test/chroma\",  \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199a0811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection: 'talks'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"talks\"\n",
    "\n",
    "# If you have created the collection before, you need to delete the collection first.\n",
    "# See https://docs.trychroma.com/usage-guide#using-collections\n",
    "if len(chroma_client.list_collections()) > 0 \\\n",
    "    and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "else:\n",
    "    print(f\"Creating collection: '{collection_name}'\")\n",
    "    talks_collection = chroma_client.create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8e29bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://docs.trychroma.com/usage-guide#adding-data-to-a-collection\n",
    "talks_list = talks[\"full_text\"].tolist()\n",
    "\n",
    "talks_collection.add(\n",
    "    documents=talks_list,\n",
    "    ids=[f\"id{x}\" for x in range(len(talks_list))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "803e0b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ids\": [\n",
      "        [\n",
      "            \"id15\",\n",
      "            \"id12\",\n",
      "            \"id4\",\n",
      "            \"id105\",\n",
      "            \"id160\",\n",
      "            \"id2\",\n",
      "            \"id174\",\n",
      "            \"id99\",\n",
      "            \"id125\",\n",
      "            \"id176\"\n",
      "        ]\n",
      "    ],\n",
      "    \"embeddings\": null,\n",
      "    \"documents\": [\n",
      "        [\n",
      "            \"Title: Deep Dive into the New Features of Apache Spark\\u2122 3.4 Abstract: In 2022, Apache Spark\\u2122 was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling.\",\n",
      "            \"Title: Use Apache Spark\\u2122 from Anywhere: Remote connectivity with Spark Connect Abstract: Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark\\u2122. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data. However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL. Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages. This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere.\",\n",
      "            \"Title: Photon for Dummies: How Does this New Execution Engine Actually Work? Abstract: Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it\\u2019s my job to understand it, explain it, and then use it. If your role involves using Apache Spark\\u2122 on Databricks, then you need to know about Photon and where to use it. Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don\\u2019t require a computer science degree. Together we will unravel mysteries such as: - Why is a Java Virtual Machine the current bottleneck for spark enhancements? - What does vectorized even mean? And how was it done before? - Why is the relationship status between Spark and Photon 'It\\u2019s complicated'? In this seession, we\\u2019ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it\\u2019s different, where the clever design choices are and how you can make the most of this in your own workloads. I\\u2019ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don\\u2019t have to.\",\n",
      "            \"Title: How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience Abstract: Disney+ uses Amazon Kinesis and Databricks Apache Spark\\u2122 streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers.\",\n",
      "            \"Title: An API for DL Inferencing on Spark Abstract: Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization. In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models.\",\n",
      "            \"Title: From Snowflake to Enterprise-Scale Apache Spark\\u2122 Abstract: Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai\\u2019s customers. The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark\\u2122 solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we\\u2019ll discuss how the mPulse team made the decision to migrate, the challenges we\\u2019ve seen and how Spark is suiting the product's needs. In the second half of the talk, we\\u2019ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to: * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation * Handle X100 queries per second on a single Spark application with sub-second query latency * Protect Spark application from misbehaving users * Optimize SQL-based queries\",\n",
      "            \"Title: Python with Spark Connect Abstract: PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python. With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today\\u2019s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example. In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications. In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what\\u2019s next beyond Apache Spark 3.4.\",\n",
      "            \"Title: Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark\\u2122 Abstract: Apache Spark\\u2122\\u2019s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important. Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa\\u2019s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data.\",\n",
      "            \"Title: Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying Abstract: Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks\\u2019 dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia\\u2019s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies, including success stories that resulted in significant cost savings and efficiency gain.\",\n",
      "            \"Title: Ray on Spark Abstract: Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale. This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change! We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases.\"\n",
      "        ]\n",
      "    ],\n",
      "    \"metadatas\": [\n",
      "        [\n",
      "            null,\n",
      "            null,\n",
      "            null,\n",
      "            null,\n",
      "            null,\n",
      "            null,\n",
      "            null,\n",
      "            null,\n",
      "            null,\n",
      "            null\n",
      "        ]\n",
      "    ],\n",
      "    \"distances\": [\n",
      "        [\n",
      "            0.9522404670715332,\n",
      "            1.0304359197616577,\n",
      "            1.0567857027053833,\n",
      "            1.1080093383789062,\n",
      "            1.108160138130188,\n",
      "            1.118524193763733,\n",
      "            1.153230905532837,\n",
      "            1.1612838506698608,\n",
      "            1.1677358150482178,\n",
      "            1.1796327829360962\n",
      "        ]\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results = talks_collection.query(query_texts=[\"Spark\"], n_results=10)\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b2fc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=lm_model, tokenizer=tokenizer, max_new_tokens=512, device_map=\"auto\", \n",
    "    handle_long_generation=\"hole\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1205a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Help me find sessions related to Spark.\"\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "requirement = \"Recommend top-5 relevant sessions for me to attend.\"\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question} {requirement}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8e5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2500 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant context: #Title: Deep Dive into the New Features of Apache Spark™ 3.4 Abstract: In 2022, Apache Spark™ was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling. #Title: Use Apache Spark™ from Anywhere: Remote connectivity with Spark Connect Abstract: Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark™. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data. However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL. Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages. This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere. #Title: Photon for Dummies: How Does this New Execution Engine Actually Work? Abstract: Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it. If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as: - Why is a Java Virtual Machine the current bottleneck for spark enhancements? - What does vectorized even mean? And how was it done before? - Why is the relationship status between Spark and Photon 'It’s complicated'? In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to. #Title: How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience Abstract: Disney+ uses Amazon Kinesis and Databricks Apache Spark™ streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers. #Title: An API for DL Inferencing on Spark Abstract: Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization. In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models. #Title: From Snowflake to Enterprise-Scale Apache Spark™ Abstract: Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers. The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™ solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs. In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to: * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation * Handle X100 queries per second on a single Spark application with sub-second query latency * Protect Spark application from misbehaving users * Optimize SQL-based queries #Title: Python with Spark Connect Abstract: PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python. With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today’s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example. In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications. In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what’s next beyond Apache Spark 3.4. #Title: Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark™ Abstract: Apache Spark™’s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important. Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa’s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data. #Title: Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying Abstract: Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks’ dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia’s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies, including success stories that resulted in significant cost savings and efficiency gain. #Title: Ray on Spark Abstract: Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale. This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change! We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases.\n",
      "\n",
      " The user's question: Help me find sessions related to Spark. Recommend top-5 relevant sessions for me to attend.\n",
      "\n",
      "The next topic for next week\n",
      "\n",
      "An introduction to the Spark cloud and the new D3 framework - Spark in Practice and how to setup\n",
      "\n",
      "We will run a discussion of D3 as a framework that can take on a new dimension of use case and then take advantage of the new power of Spark (in my own experience) to push code to its full potential.\n",
      "\n",
      "To get into learning the D3 language and see how other languages work in your development environment, check out my talk at this event.\n",
      "\n",
      "This is my first talk about this topic of my book Spark and I'll be talking in the next few weeks about building applications on D3. There are some other blogs I've read that offer some of the same information and there will be lots more about it, so if you like what you read, you can sign up on my mailing list to find out more\n",
      "\n",
      "My next talk will tackle my experience with OCaml and the D3 D3 library - the first 3D language in this series, and how to leverage these experiences to drive a significant amount of power into the next generation of Spark applications! I'll talk about the D3 D3 library and its relationship to Spark and to other technologies in this series.\n",
      "\n",
      "If you're not using D3 development now, you should not consider these talks.\n",
      "\n",
      "For more details of the content being taught at this event please visit the D3D.org website and use the comment section below.\n",
      "\n",
      "We're currently looking to add a new blog post once we've already shared the most recent content on our mailing list. We have a few more questions to ask at this event, so be sure to let us know if you have any suggestions!\n",
      "\n",
      "This event is open to anyone and anyone can be an advisor, member of a technical club or just an interested party, any questions you might have, or just want to add a comment in the comments section (if you haven't already!).\n",
      "\n",
      "You can join our event here to see a new version of our event and get updates.\n"
     ]
    }
   ],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99413f61",
   "metadata": {},
   "source": [
    "## Use OpenAI models for Q/A\n",
    "\n",
    "For this section to work, you need to generate an Open AI key. \n",
    "\n",
    "Steps:\n",
    "1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n",
    "2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "Note: OpenAI does not have a free option, but it gives you \\\\$5 as credit. Once you have exhausted your \\\\$5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). **IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93bf6f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"XXXXXXXXX\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24e8546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It would take roughly $0.0044 to run this prompt\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "price_token = 0.002\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "cost_to_run = len(encoder.encode(prompt_template)) / 1000 * price_token\n",
    "print(f\"It would take roughly ${round(cost_to_run, 5)} to run this prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f378b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "gpt35_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt_template},\n",
    "    ],\n",
    "    temperature=0, # 0 makes outputs deterministic; The closer the value is to 1, the more random the outputs are for each time you re-run.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "826ae121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are the top 5 relevant sessions related to Spark that you may want to attend:\n",
      "\n",
      "1. Deep Dive into the New Features of Apache Spark™ 3.4\n",
      "2. Use Apache Spark™ from Anywhere: Remote connectivity with Spark Connect\n",
      "3. Photon for Dummies: How Does this New Execution Engine Actually Work?\n",
      "4. How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience\n",
      "5. An API for DL Inferencing on Spark\n",
      "\n",
      "These sessions cover a range of topics related to Spark, including new features and improvements, remote connectivity, execution engine, real-time data processing, and deep learning inferencing.\n"
     ]
    }
   ],
   "source": [
    "print(gpt35_response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9e58ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure, here are the top 5 relevant sessions related to Spark that you may want to attend:\n",
       "\n",
       "1. Deep Dive into the New Features of Apache Spark™ 3.4\n",
       "2. Use Apache Spark™ from Anywhere: Remote connectivity with Spark Connect\n",
       "3. Photon for Dummies: How Does this New Execution Engine Actually Work?\n",
       "4. How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience\n",
       "5. An API for DL Inferencing on Spark\n",
       "\n",
       "These sessions cover a range of topics related to Spark, including new features and improvements, remote connectivity, execution engine, real-time data processing, and deep learning inferencing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(gpt35_response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d45d7d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2355"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_response[\"usage\"][\"total_tokens\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
