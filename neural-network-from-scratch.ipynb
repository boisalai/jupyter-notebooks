{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d3d432",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "This notebook implements a neural network without using machine learning libraries.\n",
    "\n",
    "This notebook applies code by Sylvain Gugger appearing \n",
    "in [A simple neural net in numpy](https://sgugger.github.io/a-simple-neural-net-in-numpy.html#a-simple-neural-net-in-numpy), \n",
    "posted on Tue 20 March 2018, on Iris Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d32e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "07b413cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(y):\n",
    "    \"\"\"\n",
    "    One-hot encoding\n",
    "    :param y:\n",
    "    :param unique_elements:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    unique_elements = np.unique(y)\n",
    "    nb_unique_elements = len(unique_elements)\n",
    "    onehot_encoded_y = []\n",
    "    for category in y:\n",
    "        values = [0] * nb_unique_elements\n",
    "        index = np.where(unique_elements == category)[0][0]\n",
    "        values[index] = 1\n",
    "        onehot_encoded_y.append(values)\n",
    "    \n",
    "    return np.array(onehot_encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "282442a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler:\n",
    "    \"\"\"Data standardization\"\"\"\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the mean and standard deviation of each column.\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.means = np.mean(data, axis=0)\n",
    "        self.stds = np.std(data, axis=0)\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Standardize data\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return (data - self.means) / self.stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d254bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.old_x = np.copy(x)\n",
    "        return np.clip(x, 0, None)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return np.where(self.old_x > 0, grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e32e0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.old_y = np.exp(x) / (1. + np.exp(x))\n",
    "        return self.old_y\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return self.old_y * (1. - self.old_y) * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cf1cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"Softmax activation function\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.old_y = np.exp(x) / np.exp(x).sum(axis=1)[:, None]\n",
    "        return self.old_y\n",
    "\n",
    "    def backward(self, grad):\n",
    "        return self.old_y * (grad - (grad * self.old_y).sum(axis=1)[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9981e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropy:\n",
    "    \"\"\"Cross Entropy cost\"\"\"\n",
    "    def forward(self, x, y):\n",
    "        self.old_x = x.clip(min=1e-8, max=None)\n",
    "        self.old_y = y\n",
    "        return (np.where(y == 1, -np.log(self.old_x), 0)).sum(axis=1)\n",
    "\n",
    "    def backward(self):\n",
    "        return np.where(self.old_y == 1, -1 / self.old_x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c06cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Linear Layer\"\"\"\n",
    "    def __init__(self, nb_in: int, nb_out: int):\n",
    "        self.nb_in = nb_in\n",
    "        self.nb_out = nb_out\n",
    "        self.weights = np.random.randn(nb_in, nb_out) * np.sqrt(2 / nb_in)\n",
    "        self.biases = np.zeros(nb_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.old_x = x\n",
    "        return np.dot(x, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, grad):\n",
    "        self.grad_b = grad.mean(axis=0)\n",
    "        self.grad_w = (np.matmul(self.old_x[:, :, None], grad[:, None, :])).mean(axis=0)\n",
    "        return np.dot(grad, self.weights.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6701972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Model which is the neural network\"\"\"\n",
    "    def __init__(self, layers, cost):\n",
    "        self.layers = layers\n",
    "        self.cost = cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        return self.cost.forward(self.forward(x), y)\n",
    "\n",
    "    def backward(self):\n",
    "        grad = self.cost.backward()\n",
    "        for i in range(len(self.layers) - 1, -1, -1):\n",
    "            grad = self.layers[i].backward(grad)\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"Show model structure\"\"\"\n",
    "        print(\"Sommaire du mod√®le :\")\n",
    "        nb_params = 0\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                n = layer.nb_in * layer.nb_out + layer.nb_out\n",
    "                print(f\"Linear({layer.nb_in}, {layer.nb_out}) with {n} parameters.\")\n",
    "                nb_params += n\n",
    "            else:\n",
    "                print(f\"{layer.__class__.__name__}()\")\n",
    "        print(f\"Total of {nb_params} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "19a73faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, nb_in: int, nb_out: int, nb_hn: int = 10, nb_hl: int = 2):\n",
    "        \"\"\"\n",
    "        This method creates the neural network.\n",
    "        :param nb_in: Number of input neurons.\n",
    "        :param nb_out: Number of output neurons.\n",
    "        :param nb_hn: Number of neurons in a hidden layer.\n",
    "        :param nb_hl: Number of hidden layers.\n",
    "        \"\"\"\n",
    "        self.nb_in = nb_in\n",
    "        self.nb_out = nb_out\n",
    "        self.nb_hn = nb_hn\n",
    "        self.nb_hl = nb_hl\n",
    "\n",
    "        layers = []\n",
    "        for i in range(nb_hl):\n",
    "            if i == 0:\n",
    "                layers.append(Linear(nb_in, nb_hn))\n",
    "            else:\n",
    "                layers.append(Linear(nb_hn, nb_hn))\n",
    "            layers.append(Relu())\n",
    "        layers.append(Linear(nb_hn, nb_out))\n",
    "        layers.append(Softmax())\n",
    "\n",
    "        self.model = Model(layers, CrossEntropy())\n",
    "        self.model.summary()\n",
    "        self.scaler = None\n",
    "\n",
    "    def train(self, train, train_labels, lr=1e-2, decay=0., nb_epoch=1000):\n",
    "        \"\"\"\n",
    "        Train the model on the training set.\n",
    "        :param train: \n",
    "        :param train_labels: \n",
    "        :param lr: Learning rate.\n",
    "        :param decay:\n",
    "        :param nb_epoch:\n",
    "        \"\"\"\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(train)\n",
    "        X_scaled = self.scaler.transform(train)\n",
    "        print(f\"X_scaled.shape={X_scaled.shape}\")\n",
    "\n",
    "        # One-hot encode Y.\n",
    "        print(f\"train_labels.shape={train_labels.shape}\")\n",
    "        onehot_encoded_y = onehot_encode(train_labels)\n",
    "        print(f\"onehot_encoded_y.shape={onehot_encoded_y.shape}\")\n",
    "        \n",
    "        # Concatenate X and Y matrices by columns to create the data matrix.\n",
    "        data = np.concatenate((X_scaled, onehot_encoded_y), axis=1)\n",
    "\n",
    "        # Create mini-batches of size 2 from the data matrix.\n",
    "        mini_batches = np.array_split(data, len(data) // 2)\n",
    "\n",
    "        position = self.nb_out\n",
    "        modulo = int(nb_epoch/10)\n",
    "\n",
    "        for epoch in range(nb_epoch):\n",
    "            running_loss = 0.\n",
    "            num_inputs = 0\n",
    "            for mini_batch in mini_batches:\n",
    "                inputs, targets = mini_batch[:, :-position], mini_batch[:, -position:]\n",
    "                num_inputs += inputs.shape[0]\n",
    "                # Forward pass and calculation of the loss.\n",
    "                running_loss += self.model.loss(inputs, targets).sum()\n",
    "                # Back propagation.\n",
    "                self.model.backward()\n",
    "                # Learning rate decay.\n",
    "                current_lr = lr / (1 + decay * epoch)\n",
    "\n",
    "                # Update parameters.\n",
    "                for layer in self.model.layers:\n",
    "                    if type(layer) == Linear:\n",
    "                        layer.weights -= current_lr * layer.grad_w\n",
    "                        layer.biases -= current_lr * layer.grad_b\n",
    "\n",
    "            if not epoch % modulo and False:\n",
    "                print(f'Epoch {epoch + 1}/{nb_epoch}, lr={current_lr:.4f}, loss={running_loss / num_inputs:.4f}')\n",
    "\n",
    "    def predict(self, x) -> int:\n",
    "        \"\"\"\n",
    "        Predict the class of a given example.\n",
    "        :param x: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        x_scaled = self.scaler.transform(x)\n",
    "        output = self.model.forward(x_scaled)\n",
    "        y_pred = int(np.argmax(output, axis=1))\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"\n",
    "        Evaluate the model on a set of examples without sklearn.\n",
    "        :param X: \n",
    "        :param y: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        y_pred == self.predict_all(X)\n",
    "        accuracy = sum(y_pred == y) / len(y)\n",
    "        return accuracy\n",
    "\n",
    "    def predict_all(self, X):\n",
    "        \"\"\"\n",
    "        Compute predictions on the set of attributes.\n",
    "        :param X:\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        output = self.model.forward(X_scaled)\n",
    "        y_pred = np.argmax(output, axis=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d7f2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, X, y, source):\n",
    "    \"\"\"\n",
    "    Evaluate the model and display the accuracy, precision, recall, and F1 score metrics,\n",
    "    without sklearn.\n",
    "\n",
    "    So, this function performs the following steps:\n",
    "    - Evaluate the model on the provided data (training, validation or test).\n",
    "    - Calculate metrics (confusion matrix, accuracy, precision, recall, F1-score) for each class\n",
    "      (label).\n",
    "    - Calculate the averages of these metrics over all the classes.\n",
    "    - Store metrics in a dictionary to later produce a summary table.\n",
    "\n",
    "    :param clf: Classifier.\n",
    "    :param X: Attributes of the examples.\n",
    "    :param y: Actual example labels.\n",
    "    :param source: Character string indicating the source \"train\" or \"test\".\n",
    "    :return: Dictionary containing all calculated metrics.\n",
    "    \"\"\"\n",
    "    class_name = type(clf).__name__\n",
    "    print(f\"Evaluate {class_name} on {source}\")\n",
    "\n",
    "    y_pred = clf.predict_all(X)\n",
    "    scores = dict()\n",
    "\n",
    "    for label in np.unique(y):\n",
    "        # The confusion matrix looks like this:\n",
    "        #          predicted\n",
    "        #           (+)  (-)\n",
    "        #          ---------\n",
    "        #     (+) | TP | FN |\n",
    "        # actual   ---------\n",
    "        #     (-) | FP | TN |\n",
    "        #          ---------\n",
    "        confmat = np.array(\n",
    "            [[sum([\n",
    "                ((y[i] == label) == actual) and ((y_pred[i] == label) == predicted)\n",
    "                for i in range(len(y))])\n",
    "                for actual in [True, False]]\n",
    "                for predicted in [True, False]])\n",
    "\n",
    "        # Capture values TP, FN, FP, TN.\n",
    "        tp = confmat[0, 0]\n",
    "        fn = confmat[0, 1]\n",
    "        fp = confmat[1, 0]\n",
    "        tn = confmat[1, 1]\n",
    "\n",
    "        # Calculate the requested metrics.\n",
    "        # Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        # Precision = TP / (TP + FP)\n",
    "        # Recall = TP / (TP + FN)\n",
    "        # F1 score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "        # If the denominator is zero, the metric is set to zero.\n",
    "        accuracy = 0.0 if tp + tn + fp + fn == 0 else (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = 0.0 if tp + fp == 0 else tp / (tp + fp)\n",
    "        recall = 0.0 if tp + fn == 0 else tp / (tp + fn)\n",
    "        f1 = 0.0 if precision + recall == 0 else 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "        scores[label] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "        print(f\"M√©trics for class {label} :\")\n",
    "        print(f\"- Accuracy={accuracy}, Precision={precision}, Recall={recall}, F1-score={f1}\")\n",
    "        print(f\"- Confusion matrix = \\n{np.array2string(confmat)}\")\n",
    "\n",
    "    # Calculate the averages of the metrics over all the classes.\n",
    "    n = len(scores)\n",
    "    mean_accuracy = sum([scores[label]['accuracy'] for label in scores]) / n\n",
    "    mean_precision = sum([scores[label]['precision'] for label in scores]) / n\n",
    "    mean_recall = sum([scores[label]['recall'] for label in scores]) / n\n",
    "    mean_f1 = sum([scores[label]['f1'] for label in scores]) / n\n",
    "    scores[-1] = {'accuracy': mean_accuracy, 'precision': mean_precision, 'recall': mean_recall, 'f1': mean_f1}\n",
    "\n",
    "    print(f\"Average metrics across all classes :\")\n",
    "    print(f\"- Exactitude={mean_accuracy}, Pr√©cision={mean_precision}, \"\n",
    "          f\"Rappel={mean_recall}, F1-score={mean_f1}\")\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "306a9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset and get X and Y data\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data)\n",
    "y = pd.DataFrame(iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0934abd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (120, 4)\n",
      "X_test shape: (30, 4)\n",
      "y_train shape: (120, 1)\n",
      "y_test shape: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "# set aside 20% of train and test data for evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "    test_size=0.2, shuffle = True, random_state = 123)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "78b11b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_in=4, nb_out=3, nb_hn=10, nb_hl=2\n"
     ]
    }
   ],
   "source": [
    "nb_in = X_train.shape[1]  # Number of input neurons.\n",
    "nb_out = len(np.unique(y_train))  # Number of output neurons.\n",
    "nb_hn = 10  # Number of neurons per hidden layer.\n",
    "nb_hl = 2  # Number of hidden layers.\n",
    "\n",
    "print(f\"nb_in={nb_in}, nb_out={nb_out}, nb_hn={nb_hn}, nb_hl={nb_hl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f69b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sommaire du mod√®le :\n",
      "Linear(4, 10) with 50 parameters.\n",
      "Relu()\n",
      "Linear(10, 10) with 110 parameters.\n",
      "Relu()\n",
      "Linear(10, 3) with 33 parameters.\n",
      "Softmax()\n",
      "Total de 193 param√®tres.\n",
      "X_scaled.shape=(120, 4)\n",
      "train_labels.shape=(120, 1)\n",
      "onehot_encoded_y.shape=(120, 3)\n",
      "Evaluate NeuralNet on test\n",
      "M√©trics for class 0 :\n",
      "- Accuracy=[1.], Precision=[1.], Recall=[1.], F1-score=[1.]\n",
      "- Confusion matrix = \n",
      "[[[13]\n",
      "  [ 0]]\n",
      "\n",
      " [[ 0]\n",
      "  [17]]]\n",
      "M√©trics for class 1 :\n",
      "- Accuracy=[0.93333333], Precision=[1.], Recall=[0.75], F1-score=[0.85714286]\n",
      "- Confusion matrix = \n",
      "[[[ 6]\n",
      "  [ 2]]\n",
      "\n",
      " [[ 0]\n",
      "  [22]]]\n",
      "M√©trics for class 2 :\n",
      "- Accuracy=[0.93333333], Precision=[0.81818182], Recall=[1.], F1-score=[0.9]\n",
      "- Confusion matrix = \n",
      "[[[ 9]\n",
      "  [ 0]]\n",
      "\n",
      " [[ 2]\n",
      "  [19]]]\n",
      "Average metrics across all classes :\n",
      "- Exactitude=[0.95555556], Pr√©cision=[0.93939394], Rappel=[0.91666667], F1-score=[0.91904762]\n"
     ]
    }
   ],
   "source": [
    "clf = NeuralNet(nb_in, nb_out, nb_hn, nb_hl)\n",
    "clf.train(X_train, y_train)\n",
    "scores = evaluate(clf, X_test, y_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f3796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
